/**
 * BioPassport Comprehensive Benchmark Suite
 * 
 * IEEE ICBC Best Paper-level evaluation:
 * - Latency distributions (p50/p95/p99)
 * - Throughput vs concurrency
 * - Storage growth analysis (EVM slots)
 * - Confusion matrices (system predictions vs ground truth)
 * - Baseline comparisons
 * - Ablation studies
 * 
 * Modes:
 *   --live      Real PureChain transactions (DEFAULT for paper results)
 *   --simulate  Emulated latencies (for development/testing only)
 *   --full      Extended iterations and concurrency levels
 * 
 * Run with: npx ts-node benchmark-suite.ts --live [--full]
 */

import * as fs from 'fs';
import * as path from 'path';
import * as crypto from 'crypto';
import { performance } from 'perf_hooks';

// ==================== Blockchain Client Interface ====================

interface BlockchainClient {
  connect(): Promise<void>;
  disconnect(): Promise<void>;
  deployContract(): Promise<string>;
  registerMaterial(materialType: string, metadataHash: string): Promise<{ latencyMs: number; materialId: string }>;
  // credType: 'IDENTITY' | 'QC_MYCO' | 'USAGE_RIGHTS' (string, not enum index)
  issueCredential(materialId: string, credType: string, commitmentHash: string, validUntil: number, artifactCid: string, artifactHash: string, signatureRef: string): Promise<{ latencyMs: number; credentialId: string }>;
  // Returns transferId which must be used for acceptTransfer
  initiateTransfer(materialId: string, toOrg: string, shipmentHash: string): Promise<{ latencyMs: number; transferId: string }>;
  // CRITICAL: Takes transferId, NOT materialId
  acceptTransfer(transferId: string): Promise<{ latencyMs: number }>;
  verifyMaterial(materialId: string): Promise<{ latencyMs: number; pass: boolean; reasons: string[] }>;
  // Note: Currently returns full history, not a true slice.
  getHistory(materialId: string): Promise<{ latencyMs: number; count: number }>;
  // Status change methods - require reasonHash parameter
  setStatus(materialId: string, status: 'ACTIVE' | 'QUARANTINED' | 'REVOKED', reasonHash: string): Promise<{ latencyMs: number }>;
}

// Global benchmark mode
let BENCHMARK_MODE: 'live' | 'simulate' = 'simulate';

// ==================== Types ====================

interface LatencyStats {
  count: number;
  min: number;
  max: number;
  mean: number;
  p50: number;
  p95: number;
  p99: number;
  stdDev: number;
}

interface ThroughputResult {
  concurrency: number;
  opsPerSecond: number;
  avgLatencyMs: number;
  p99LatencyMs: number;
}

interface StorageMetrics {
  materialsCount: number;
  avgCredentialsPerMaterial: number;
  avgTransfersPerMaterial: number;
  slotsPerMaterial: number;      // EVM storage slots (32 bytes each)
  slotsPerCredential: number;
  slotsPerTransfer: number;
  totalSlots: number;
  totalBytes: number;            // totalSlots * 32
}

interface ConfusionMatrix {
  truePositive: number;   // Correctly identified as FAIL
  trueNegative: number;   // Correctly identified as PASS
  falsePositive: number;  // Incorrectly flagged as FAIL
  falseNegative: number;  // Incorrectly passed as PASS (missed attack)
  tpr: number;            // True Positive Rate (Sensitivity/Recall)
  tnr: number;            // True Negative Rate (Specificity)
  fpr: number;            // False Positive Rate
  fnr: number;            // False Negative Rate
  precision: number;
  accuracy: number;
  f1Score: number;
}

interface AnomalyDetectionResult {
  anomalyType: string;
  confusionMatrix: ConfusionMatrix;
}

interface BaselineResult {
  name: string;
  description: string;
  latencyMs: LatencyStats;
  throughputOps: number;
  securityScore: number;  // 0-100
  integrityGuarantee: string;
}

interface AblationResult {
  name: string;
  description: string;
  featureDisabled: string;
  baselinePassRate: number;
  ablatedPassRate: number;
  falseAcceptIncrease: number;
  securityImpact: string;
}

interface ReproducibilityMetadata {
  gitCommitHash: string;
  nodeVersion: string;
  platform: string;
  benchmarkMode: string;
  chainId?: string;
  networkName?: string;
  rpcUrl?: string;  // Redacted for security
  datasetChecksum: string;
}

interface BenchmarkReport {
  timestamp: string;
  reproducibility: ReproducibilityMetadata;
  config: {
    materialsCount: number;
    concurrencyLevels: number[];
    iterations: number;
  };
  latency: {
    registerMaterial: LatencyStats;
    issueIdentity: LatencyStats;
    issueQC: LatencyStats;
    initiateTransfer: LatencyStats;
    acceptTransfer: LatencyStats;
    verifyMaterialOnChain: LatencyStats;
    verifyMaterialFull: LatencyStats;
  };
  throughput: ThroughputResult[];
  storage: StorageMetrics;
  confusionMatrices: {
    onChain: AnomalyDetectionResult[];
    full: AnomalyDetectionResult[];
  };
  baselines: BaselineResult[];
  ablations: AblationResult[];
  scalingTest: {
    materialCounts: number[];
    verifyLatencyMs: number[];
    queryLatencyMs: number[];
  };
}

// ==================== Utilities ====================

function calculateStats(values: number[]): LatencyStats {
  if (values.length === 0) {
    return { count: 0, min: 0, max: 0, mean: 0, p50: 0, p95: 0, p99: 0, stdDev: 0 };
  }
  
  const sorted = [...values].sort((a, b) => a - b);
  const count = sorted.length;
  const min = sorted[0];
  const max = sorted[count - 1];
  const sum = sorted.reduce((a, b) => a + b, 0);
  const mean = sum / count;
  
  // Correct percentile calculation: use (count - 1) * p for 0-indexed array
  const p50 = sorted[Math.floor((count - 1) * 0.50)];
  const p95 = sorted[Math.floor((count - 1) * 0.95)];
  const p99 = sorted[Math.floor((count - 1) * 0.99)];
  
  const squaredDiffs = sorted.map(v => Math.pow(v - mean, 2));
  const variance = squaredDiffs.reduce((a, b) => a + b, 0) / count;
  const stdDev = Math.sqrt(variance);
  
  return { count, min, max, mean, p50, p95, p99, stdDev };
}

function simulateLatency(baseMs: number, varianceMs: number): number {
  // Simulate realistic latency with some variance
  const jitter = (Math.random() - 0.5) * 2 * varianceMs;
  const spike = Math.random() < 0.05 ? baseMs * (1 + Math.random()) : 0; // 5% chance of spike
  return Math.max(1, baseMs + jitter + spike);
}

function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// Safe accessors for dataset fields (prevent crashes on malformed data)
function creds(m: any): any[] { return Array.isArray(m?.credentials) ? m.credentials : []; }
function transfers(m: any): any[] { return Array.isArray(m?.transfers) ? m.transfers : []; }

// Single-writer queue for serializing write operations (prevents nonce collisions)
class WriteQueue {
  private queue: Promise<any> = Promise.resolve();
  
  async enqueue<T>(fn: () => Promise<T>): Promise<T> {
    const result = this.queue.then(fn, fn); // Run even if previous failed
    this.queue = result.catch(() => {}); // Prevent unhandled rejection
    return result;
  }
}

// ==================== Latency Benchmarks ====================

async function benchmarkLatency(iterations: number, client?: BlockchainClient, dataDir?: string): Promise<BenchmarkReport['latency']> {
  const isLive = BENCHMARK_MODE === 'live' && client;
  console.log(`\nüìä Running Latency Benchmarks (${isLive ? 'LIVE' : 'SIMULATED'})...`);
  
  // Default dataDir for artifact lookups
  const artifactDir = dataDir || path.join(__dirname, 'data');
  
  const results = {
    registerMaterial: [] as number[],
    issueIdentity: [] as number[],
    issueQC: [] as number[],
    initiateTransfer: [] as number[],
    acceptTransfer: [] as number[],
    verifyMaterialOnChain: [] as number[],
    verifyMaterialFull: [] as number[],
  };
  
  if (isLive && client) {
    // LIVE MODE: Measure actual PureChain transactions
    console.log('  [LIVE] Measuring actual transaction latencies...');
    
    for (let i = 0; i < iterations; i++) {
      const hash = crypto.createHash('sha256').update(`latency-test-${i}`).digest('hex');
      
      // Register material
      const regResult = await client.registerMaterial('CELL_LINE', hash);
      results.registerMaterial.push(regResult.latencyMs);
      
      // Issue IDENTITY credential (required for valid verification path)
      const identityResult = await client.issueCredential(
        regResult.materialId, 'IDENTITY', hash, 
        Math.floor(Date.now() / 1000) + 86400 * 90,
        `s3://test/${i}`, hash, ''
      );
      results.issueIdentity.push(identityResult.latencyMs);
      
      // Issue QC credential (required for valid verification path)
      const qcResult = await client.issueCredential(
        regResult.materialId, 'QC_MYCO', hash, 
        Math.floor(Date.now() / 1000) + 86400 * 90,
        `s3://test/${i}-qc`, hash, ''
      );
      results.issueQC.push(qcResult.latencyMs);
      
      // Verify (on-chain only) - single call, reuse for full verification
      // Now material has IDENTITY + QC, so we measure realistic verification path
      const verifyResult = await client.verifyMaterial(regResult.materialId);
      
      // SANITY CHECK: First iteration - verify credentials are recognized
      if (i === 0) {
        console.log(`  [SANITY] Material 0: pass=${verifyResult.pass}, reasons=${JSON.stringify(verifyResult.reasons)}`);
      }
      results.verifyMaterialOnChain.push(verifyResult.latencyMs);
      
      // Full verification: reuse on-chain latency + measured artifact integrity check
      // This avoids double-verify inflation by not calling verifyMaterial twice
      const artifactCheckStart = performance.now();
      // PAPER-GRADE: Always use deterministic test artifact for reproducibility
      // This measures pure hashing cost without disk I/O variance
      // Label in paper as "hashing cost only, no retrieval latency"
      const artifactBytes = Buffer.alloc(4096);
      const seed = crypto.createHash('sha256').update(regResult.materialId).digest();
      seed.copy(artifactBytes, 0, 0, Math.min(seed.length, artifactBytes.length));
      // Hash the artifact (actual integrity check)
      crypto.createHash('sha256').update(artifactBytes).digest('hex');
      const artifactCheckLatency = performance.now() - artifactCheckStart;
      
      // Full = on-chain + artifact check (no double-verify)
      results.verifyMaterialFull.push(verifyResult.latencyMs + artifactCheckLatency);
      
      // Transfer operations (every 10th iteration to avoid too many)
      if (i % 10 === 0) {
        // initiateTransfer returns transferId which must be used for acceptTransfer
        const transferResult = await client.initiateTransfer(
          regResult.materialId, 'OtherOrg', hash
        );
        results.initiateTransfer.push(transferResult.latencyMs);
        
        // CRITICAL: Use transferId, not materialId
        const acceptResult = await client.acceptTransfer(transferResult.transferId);
        results.acceptTransfer.push(acceptResult.latencyMs);
      }
      
      if ((i + 1) % 50 === 0) {
        console.log(`  ${i + 1}/${iterations} completed`);
      }
    }
  } else {
    // SIMULATE MODE: Use realistic latency distributions
    console.log('  [SIMULATED] Using emulated latency distributions...');
    
    for (let i = 0; i < iterations; i++) {
      // Based on typical permissioned blockchain performance
      results.registerMaterial.push(simulateLatency(45, 15));      // ~45ms avg
      results.issueIdentity.push(simulateLatency(38, 12));         // ~38ms avg
      results.issueQC.push(simulateLatency(40, 13));               // ~40ms avg
      results.verifyMaterialOnChain.push(simulateLatency(8, 3));   // ~8ms avg (read-only)
      results.verifyMaterialFull.push(simulateLatency(25, 10));    // ~25ms avg (includes artifact check)
      
      // Transfer operations sampled every 10th iteration (same as LIVE mode)
      if (i % 10 === 0) {
        results.initiateTransfer.push(simulateLatency(42, 14));    // ~42ms avg
        results.acceptTransfer.push(simulateLatency(40, 13));      // ~40ms avg
      }
      
      if ((i + 1) % 100 === 0) {
        process.stdout.write(`  Iteration ${i + 1}/${iterations}\r`);
      }
    }
  }
  
  console.log(`  Completed ${iterations} iterations`);
  
  // Note: Transfer operations have ~iterations/10 samples (every 10th iteration)
  const transferSampleSize = Math.ceil(iterations / 10);
  console.log(`  NOTE: Transfer ops sampled at 1/10 rate (n=${transferSampleSize})`);
  
  return {
    registerMaterial: calculateStats(results.registerMaterial),
    issueIdentity: calculateStats(results.issueIdentity),
    issueQC: calculateStats(results.issueQC),
    initiateTransfer: calculateStats(results.initiateTransfer),  // n = iterations/10
    acceptTransfer: calculateStats(results.acceptTransfer),      // n = iterations/10
    verifyMaterialOnChain: calculateStats(results.verifyMaterialOnChain),
    verifyMaterialFull: calculateStats(results.verifyMaterialFull),
  };
}

// ==================== Throughput Benchmarks ====================

async function benchmarkThroughput(concurrencyLevels: number[], client?: BlockchainClient): Promise<ThroughputResult[]> {
  const isLive = BENCHMARK_MODE === 'live' && client;
  console.log(`\nüìà Running Throughput Benchmarks (${isLive ? 'LIVE' : 'SIMULATED'})...`);
  
  const results: ThroughputResult[] = [];
  
  // For live mode, pre-seed a FIXED pool of valid materialIds for read operations
  // Pool is frozen during throughput runs to avoid bias from growing pool size
  // CRITICAL: Each material must have IDENTITY + QC credentials for realistic verify path
  const materialIdPool: string[] = [];
  if (isLive && client) {
    console.log('  Pre-seeding material pool with IDENTITY + QC credentials...');
    const poolSize = 50;
    for (let i = 0; i < poolSize; i++) {
      const hash = crypto.createHash('sha256').update(`pool-seed-${i}-${Date.now()}`).digest('hex');
      const result = await client.registerMaterial('CELL_LINE', hash);
      
      // Issue IDENTITY credential (required for valid verification)
      await client.issueCredential(
        result.materialId, 'IDENTITY', hash,
        Math.floor(Date.now() / 1000) + 86400 * 90,
        `s3://pool/${i}`, hash, ''
      );
      
      // Issue QC credential (required for valid verification)
      await client.issueCredential(
        result.materialId, 'QC_MYCO', hash,
        Math.floor(Date.now() / 1000) + 86400 * 90,
        `s3://pool/${i}-qc`, hash, ''
      );
      
      materialIdPool.push(result.materialId);
    }
    console.log(`  Created ${poolSize} materials with credentials for read pool (frozen during test)`);
  }
  
  for (const concurrency of concurrencyLevels) {
    const opsPerClient = isLive ? 20 : 100; // Fewer ops in live mode
    const clientLatencies: number[][] = Array(concurrency).fill(null).map(() => []);
    
    // Collect new materialIds during run, but don't add to read pool until after
    const newMaterialIds: string[] = [];
    
    const startTime = Date.now();
    
    if (isLive && client) {
      // LIVE MODE: Actual concurrent transactions
      // Read pool is FROZEN during the run to avoid bias
      const frozenPoolSize = materialIdPool.length;
      
      // Single-writer queue to serialize writes (prevents nonce collisions)
      const writeQueue = new WriteQueue();
      
      const promises = Array(concurrency).fill(0).map(async (_, clientIdx) => {
        for (let i = 0; i < opsPerClient; i++) {
          const opStart = performance.now();
          
          // Mixed workload (70% reads, 30% writes)
          if (Math.random() < 0.7) {
            // Read: verify a random material from the FROZEN pool (concurrent OK)
            const poolIdx = Math.floor(Math.random() * frozenPoolSize);
            await client.verifyMaterial(materialIdPool[poolIdx]);
          } else {
            // Write: serialize ALL writes (register + credentials) to prevent nonce collisions
            const hash = crypto.createHash('sha256').update(`throughput-${clientIdx}-${i}-${Date.now()}`).digest('hex');

            const newId = await writeQueue.enqueue(async () => {
              const reg = await client.registerMaterial('CELL_LINE', hash);

              // Issue IDENTITY + QC so future verify reads are realistic (not fast-fail)
              await client.issueCredential(
                reg.materialId, 'IDENTITY', hash,
                Math.floor(Date.now() / 1000) + 86400 * 90,
                `s3://throughput/${clientIdx}/${i}`, hash, ''
              );

              await client.issueCredential(
                reg.materialId, 'QC_MYCO', hash,
                Math.floor(Date.now() / 1000) + 86400 * 90,
                `s3://throughput/${clientIdx}/${i}-qc`, hash, ''
              );

              return reg.materialId;
            });

            newMaterialIds.push(newId);
          }
          
          clientLatencies[clientIdx].push(performance.now() - opStart);
        }
      });
      
      await Promise.all(promises);
      
      // Add new materials to pool AFTER the run completes
      materialIdPool.push(...newMaterialIds);
    } else {
      // SIMULATE MODE: Emulated concurrent operations
      const promises = Array(concurrency).fill(0).map(async (_, clientIdx) => {
        for (let i = 0; i < opsPerClient; i++) {
          const opStart = Date.now();
          // Simulate mixed workload (70% reads, 30% writes)
          if (Math.random() < 0.7) {
            await sleep(simulateLatency(8, 3)); // Read
          } else {
            await sleep(simulateLatency(42, 14)); // Write
          }
          clientLatencies[clientIdx].push(Date.now() - opStart);
        }
      });
      
      await Promise.all(promises);
    }
    
    const totalTime = (Date.now() - startTime) / 1000;
    const totalOps = concurrency * opsPerClient;
    const opsPerSecond = totalOps / totalTime;
    
    // Merge all client latencies
    const allLatencies = clientLatencies.flat();
    const sorted = [...allLatencies].sort((a, b) => a - b);
    const avgLatencyMs = allLatencies.reduce((a, b) => a + b, 0) / allLatencies.length;
    const p99LatencyMs = sorted[Math.floor((sorted.length - 1) * 0.99)];
    
    results.push({
      concurrency,
      opsPerSecond: Math.round(opsPerSecond * 100) / 100,  // 2 d.p.
      avgLatencyMs: Math.round(avgLatencyMs * 100) / 100,
      p99LatencyMs: Math.round(p99LatencyMs * 100) / 100,
    });
    
    console.log(`  Concurrency ${concurrency}: ${opsPerSecond.toFixed(2)} ops/sec, p99=${p99LatencyMs.toFixed(1)}ms`);
  }
  
  return results;
}

// ==================== Storage Analysis ====================

/**
 * Storage analysis for BioPassport contract.
 * 
 * IMPORTANT: The current contract uses dynamic strings for IDs (materialId, 
 * credentialId, issuerId, etc.), NOT bytes32. This means storage is variable
 * and depends on string lengths.
 * 
 * For accurate measurement, we:
 * 1. Calculate based on actual string lengths in the dataset
 * 2. Account for Solidity string storage overhead (length prefix + padding)
 * 3. Report both estimated slots and event log growth
 * 
 * Solidity string storage:
 * - Short strings (‚â§31 bytes): 1 slot (length in low byte, data in remaining)
 * - Long strings (>31 bytes): 1 slot for length + ceil(length/32) slots for data
 * 
 * NOTE: For a production best-paper claim, consider refactoring contract to use
 * bytes32 hashes on-chain with human-readable IDs only in events.
 */
function analyzeStorage(dataDir: string): StorageMetrics {
  console.log('\nüíæ Analyzing Storage Metrics (Dynamic Strings)...');
  console.log('  NOTE: Contract uses dynamic strings; estimates based on actual data lengths');
  
  const materialsPath = path.join(dataDir, 'normal', 'materials.json');
  const materials = JSON.parse(fs.readFileSync(materialsPath, 'utf8'));
  
  let totalCredentials = 0;
  let totalTransfers = 0;
  let totalMaterialSlots = 0;
  let totalCredentialSlots = 0;
  let totalTransferSlots = 0;
  
  for (const m of materials) {
    // Material struct slots (with dynamic strings):
    // - materialId: string (1-2 slots typically for "bio:cell_line:123")
    // - materialType: string (1 slot for "CELL_LINE" or "PLASMID")
    // - metadataHash: string (2 slots for 64-char hex hash)
    // - ownerOrg: string (1 slot typically)
    // - status: uint8 (packed)
    // - createdAt, updatedAt: uint256 (2 slots)
    const materialIdSlots = calculateStringSlots(m.materialId || 'bio:cell_line:1');
    const materialTypeSlots = calculateStringSlots(m.materialType || 'CELL_LINE');
    const metadataHashSlots = calculateStringSlots(m.metadataHash || '');
    const ownerOrgSlots = calculateStringSlots(m.ownerOrg || 'Org1');
    const materialFixedSlots = 3; // status + timestamps + mapping overhead
    totalMaterialSlots += materialIdSlots + materialTypeSlots + metadataHashSlots + ownerOrgSlots + materialFixedSlots;
    
    for (const c of creds(m)) {
      // Credential struct slots:
      // - credentialId, materialId, issuerId: strings
      // - credentialType: string
      // - commitmentHash: string (64-char hex)
      // - artifactCid: string (variable)
      // - validUntil, issuedAt: uint256
      // - revoked: bool
      const credIdSlots = calculateStringSlots(c.credentialId || 'cred:1');
      const credMatIdSlots = calculateStringSlots(c.materialId || m.materialId);
      const credTypeSlots = calculateStringSlots(c.credentialType || 'QC_MYCO');
      const issuerIdSlots = calculateStringSlots(c.issuerId || 'Issuer1');
      const commitmentSlots = calculateStringSlots(c.commitmentHash || '');
      const artifactSlots = calculateStringSlots(c.artifactRef?.cid || 's3://bucket/file');
      const credFixedSlots = 4; // timestamps + revoked + mapping overhead
      totalCredentialSlots += credIdSlots + credMatIdSlots + credTypeSlots + issuerIdSlots + commitmentSlots + artifactSlots + credFixedSlots;
      totalCredentials++;
    }
    
    for (const t of transfers(m)) {
      // Transfer struct slots:
      // - fromOrg, toOrg: strings
      // - shipmentHash: string
      // - timestamp: uint256
      // - accepted: bool
      const fromSlots = calculateStringSlots(t.fromOrg || 'Org1');
      const toSlots = calculateStringSlots(t.toOrg || 'Org2');
      const shipmentSlots = calculateStringSlots(t.shipmentHash || '');
      const transferFixedSlots = 2; // timestamp + accepted
      totalTransferSlots += fromSlots + toSlots + shipmentSlots + transferFixedSlots;
      totalTransfers++;
    }
  }
  
  const totalSlots = totalMaterialSlots + totalCredentialSlots + totalTransferSlots;
  
  // Calculate averages
  const avgSlotsPerMaterial = materials.length > 0 ? Math.round(totalMaterialSlots / materials.length * 10) / 10 : 0;
  const avgSlotsPerCredential = totalCredentials > 0 ? Math.round(totalCredentialSlots / totalCredentials * 10) / 10 : 0;
  const avgSlotsPerTransfer = totalTransfers > 0 ? Math.round(totalTransferSlots / totalTransfers * 10) / 10 : 0;
  
  const metrics: StorageMetrics = {
    materialsCount: materials.length,
    avgCredentialsPerMaterial: materials.length > 0 ? Math.round(totalCredentials / materials.length * 100) / 100 : 0,
    avgTransfersPerMaterial: materials.length > 0 ? Math.round(totalTransfers / materials.length * 100) / 100 : 0,
    slotsPerMaterial: avgSlotsPerMaterial,
    slotsPerCredential: avgSlotsPerCredential,
    slotsPerTransfer: avgSlotsPerTransfer,
    totalSlots: totalSlots,
    totalBytes: totalSlots * 32,
  };
  
  console.log(`  Materials: ${metrics.materialsCount}`);
  console.log(`  Avg credentials/material: ${metrics.avgCredentialsPerMaterial}`);
  console.log(`  Avg slots/material: ${metrics.slotsPerMaterial} (~${Math.round(metrics.slotsPerMaterial * 32)} bytes)`);
  console.log(`  Avg slots/credential: ${metrics.slotsPerCredential} (~${Math.round(metrics.slotsPerCredential * 32)} bytes)`);
  console.log(`  Total slots: ${metrics.totalSlots} (${(metrics.totalBytes / 1024).toFixed(2)} KB)`);
  console.log('  ‚ö†Ô∏è  Estimates based on dynamic string lengths; actual may vary');
  
  return metrics;
}

/**
 * Calculate EVM storage slots needed for a Solidity string.
 * 
 * Solidity string storage layout:
 * - Short strings (‚â§31 bytes): 1 slot (length in low byte, data in remaining 31 bytes)
 * - Long strings (>31 bytes): 
 *   - 1 main slot (stores length*2+1 marker)
 *   - ceil(len/32) data slots at keccak256(slot)
 *   Total: 1 + ceil(len/32) slots
 */
function calculateStringSlots(s: string): number {
  // Even empty/null string occupies 1 slot (short-string encoding)
  if (s === undefined || s === null || s === '') return 1;
  
  const len = Buffer.byteLength(s, 'utf8');
  
  // Short string: stored inline in single slot
  if (len <= 31) return 1;
  
  // Long string: 1 main slot + ceil(len/32) data slots
  return 1 + Math.ceil(len / 32);
}

/**
 * Normalize a date field to unix timestamp (seconds).
 * Handles: unix seconds, unix milliseconds, ISO string, Date object.
 * This ensures consistent comparison regardless of dataset format.
 */
function normalizeToUnixSeconds(dateField: any): number {
  if (!dateField) return 0;
  if (typeof dateField === 'number') {
    // If > 1e12, it's milliseconds; convert to seconds
    return dateField > 1e12 ? Math.floor(dateField / 1000) : dateField;
  }
  if (typeof dateField === 'string') {
    const parsed = Date.parse(dateField);
    return isNaN(parsed) ? 0 : Math.floor(parsed / 1000);
  }
  if (dateField instanceof Date) {
    return Math.floor(dateField.getTime() / 1000);
  }
  return 0;
}

/**
 * Compare two date fields, returning true if a > b.
 * Uses normalizeToUnixSeconds for consistent comparison.
 */
function isDateAfter(a: any, b: any): boolean {
  return normalizeToUnixSeconds(a) > normalizeToUnixSeconds(b);
}

// ==================== Confusion Matrices ====================

/**
 * Calculate confusion matrices using SYSTEM PREDICTIONS vs GROUND TRUTH
 * 
 * Two modes:
 * 1. Logic-based (default): Simulates verifier behavior without contract calls
 * 2. Live (--live-confusion): Actually materializes data onto chain and queries
 *    real contract outputs for maximum credibility
 * 
 * Ground truth: dataset generator's known anomalies
 * Predictions: contract verifyMaterial() output (live) or simulated logic (default)
 */
async function calculateConfusionMatrices(
  dataDir: string, 
  client?: BlockchainClient,
  liveConfusion: boolean = false
): Promise<BenchmarkReport['confusionMatrices']> {
  
  if (liveConfusion && client) {
    return await calculateLiveConfusionMatrices(dataDir, client);
  }
  
  console.log('\nüéØ Calculating Confusion Matrices (Logic-based Predictions vs Ground Truth)...');
  console.log('  NOTE: Using simulated verification logic; for live contract calls, use --live-confusion');
  
  const anomalyTypes = [
    'QC_EXPIRED',
    'QC_MISSING',
    'MATERIAL_REVOKED',
    'MATERIAL_QUARANTINED',
    'TRANSFER_PENDING',
    'ARTIFACT_TAMPERED',
  ];
  
  const onChainResults: AnomalyDetectionResult[] = [];
  const fullResults: AnomalyDetectionResult[] = [];
  
  // Load all datasets
  const datasets = ['normal', 'drift', 'adversarial'];
  const allMaterials: any[] = [];
  
  for (const ds of datasets) {
    const materialsPath = path.join(dataDir, ds, 'materials.json');
    if (fs.existsSync(materialsPath)) {
      const materials = JSON.parse(fs.readFileSync(materialsPath, 'utf8'));
      allMaterials.push(...materials);
    }
  }
  
  console.log(`  Analyzing ${allMaterials.length} materials across ${datasets.length} datasets`);
  
  // For each anomaly type, compute confusion matrix
  for (const anomalyType of anomalyTypes) {
    // === ON-CHAIN DETECTION ===
    // Simulate contract verifyMaterial() - returns (pass, reasons[])
    let tp = 0, tn = 0, fp = 0, fn = 0;
    
    for (const m of allMaterials) {
      // Ground truth: does this material actually have this anomaly?
      const groundTruthPositive = detectGroundTruthAnomaly(m, anomalyType);
      
      // Prediction: would the on-chain verifier detect this specific anomaly?
      const onChainPrediction = simulateOnChainDetection(m, anomalyType);
      
      if (groundTruthPositive && onChainPrediction) tp++;
      else if (!groundTruthPositive && !onChainPrediction) tn++;
      else if (!groundTruthPositive && onChainPrediction) fp++;
      else if (groundTruthPositive && !onChainPrediction) fn++;
    }
    
    onChainResults.push({
      anomalyType,
      confusionMatrix: buildConfusionMatrix(tp, tn, fp, fn),
    });
    
    // === FULL VERIFICATION DETECTION ===
    // Simulate verifier CLI - includes artifact integrity check
    tp = 0; tn = 0; fp = 0; fn = 0;
    
    for (const m of allMaterials) {
      const groundTruthPositive = detectGroundTruthAnomaly(m, anomalyType);
      const fullPrediction = simulateFullDetection(m, anomalyType);
      
      if (groundTruthPositive && fullPrediction) tp++;
      else if (!groundTruthPositive && !fullPrediction) tn++;
      else if (!groundTruthPositive && fullPrediction) fp++;
      else if (groundTruthPositive && !fullPrediction) fn++;
    }
    
    fullResults.push({
      anomalyType,
      confusionMatrix: buildConfusionMatrix(tp, tn, fp, fn),
    });
  }
  
  // Also compute BINARY validity classification (PASS/FAIL overall)
  console.log('\n  === Binary Validity Classification ===');
  const binaryOnChain = computeBinaryClassification(allMaterials, 'onchain');
  const binaryFull = computeBinaryClassification(allMaterials, 'full');
  
  console.log(`  On-chain: Accuracy=${(binaryOnChain.accuracy * 100).toFixed(1)}%, TPR=${(binaryOnChain.tpr * 100).toFixed(1)}%, FPR=${(binaryOnChain.fpr * 100).toFixed(1)}%`);
  console.log(`  Full:     Accuracy=${(binaryFull.accuracy * 100).toFixed(1)}%, TPR=${(binaryFull.tpr * 100).toFixed(1)}%, FPR=${(binaryFull.fpr * 100).toFixed(1)}%`);
  
  // Print per-anomaly summary
  console.log('\n  === Per-Anomaly Detection (Reason-level) ===');
  console.log('  On-chain Detection:');
  for (const r of onChainResults) {
    const m = r.confusionMatrix;
    console.log(`    ${r.anomalyType.padEnd(20)}: TPR=${(m.tpr * 100).toFixed(1).padStart(5)}%, FPR=${(m.fpr * 100).toFixed(1).padStart(5)}%, F1=${(m.f1Score * 100).toFixed(1).padStart(5)}%`);
  }
  
  console.log('  Full Verification:');
  for (const r of fullResults) {
    const m = r.confusionMatrix;
    console.log(`    ${r.anomalyType.padEnd(20)}: TPR=${(m.tpr * 100).toFixed(1).padStart(5)}%, FPR=${(m.fpr * 100).toFixed(1).padStart(5)}%, F1=${(m.f1Score * 100).toFixed(1).padStart(5)}%`);
  }
  
  return { onChain: onChainResults, full: fullResults };
}

/**
 * LIVE confusion matrix calculation - actually materializes data onto chain
 * and queries real contract outputs.
 * 
 * This is the "best-paper" approach: real system outputs vs ground truth.
 */
async function calculateLiveConfusionMatrices(
  dataDir: string,
  client: BlockchainClient
): Promise<BenchmarkReport['confusionMatrices']> {
  console.log('\nüéØ Calculating Confusion Matrices (LIVE - Actual Contract Outputs)...');
  console.log('  This materializes dataset onto chain and queries real verifyMaterial() outputs');
  
  const anomalyTypes = [
    'QC_EXPIRED',
    'QC_MISSING', 
    'MATERIAL_REVOKED',
    'MATERIAL_QUARANTINED',
    'TRANSFER_PENDING',
    'ARTIFACT_TAMPERED',
  ];
  
  // Load adversarial dataset (most interesting for confusion matrices)
  const materialsPath = path.join(dataDir, 'adversarial', 'materials.json');
  if (!fs.existsSync(materialsPath)) {
    console.log('  ‚ö†Ô∏è  No adversarial dataset found, falling back to logic-based');
    return calculateConfusionMatrices(dataDir);
  }
  
  const materials = JSON.parse(fs.readFileSync(materialsPath, 'utf8'));
  console.log(`  Materializing ${materials.length} materials onto chain...`);
  
  // Materialize each material onto chain and collect predictions
  // CRITICAL: Must reproduce ALL dataset state including status, transfers, QC ordering
  const predictions: Array<{
    material: any;
    materialId: string;
    onChainPass: boolean;
    onChainReasons: string[];
    fullPass: boolean;
    fullReasons: string[];
  }> = [];
  
  // Map dataset materialId to on-chain materialId for artifact lookups
  const materialIdMap: Map<string, string> = new Map();
  
  for (let i = 0; i < materials.length; i++) {
    const m = materials[i];
    
    try {
      // Register material
      const metadataHash = crypto.createHash('sha256').update(JSON.stringify(m)).digest('hex');
      const regResult = await client.registerMaterial(m.materialType || 'CELL_LINE', metadataHash);
      const materialId = regResult.materialId;
      
      // Store mapping for artifact lookups
      materialIdMap.set(m.materialId || `dataset-${i}`, materialId);
      
      // === Issue credentials in correct order for QC replay scenarios ===
      // IMPORTANT: QC expiry semantics are ORDER-BASED, not timestamp-based.
      // The contract uses block.timestamp for issuedAt, so we cannot reproduce
      // exact dataset timestamps. Instead, we preserve ISSUANCE ORDER:
      // - Sort credentials by dataset issuedAt
      // - Issue in that order so contract's "latest" matches dataset's "latest"
      // - Ground truth "latest QC expired" = last QC in sorted list has expired flag
      const sortedCreds = [...creds(m)].sort((a, b) => 
        normalizeToUnixSeconds(a.issuedAt) - normalizeToUnixSeconds(b.issuedAt)
      );
      
      for (const cred of sortedCreds) {
        // credType is now a string, not enum index
        const credType = cred.credentialType || 'IDENTITY';
        
        // For QC replay: older valid QC first, then newer expired QC
        // Contract uses block timestamp for ordering, so issue order matters
        const validUntil = cred.expired ? 
          Math.floor(Date.now() / 1000) - 86400 : // Expired: 1 day ago
          Math.floor(Date.now() / 1000) + 86400 * 90; // Valid: 90 days
        
        const credResult = await client.issueCredential(
          materialId,
          credType,
          cred.commitmentHash || metadataHash,
          validUntil,
          cred.artifactRef?.cid || 's3://test',
          cred.artifactRef?.hash || metadataHash,
          ''  // signatureRef
        );
        // Debug: log first few credential issuances to verify they're working
        if (i < 3) {
          console.log(`    [DEBUG] Material ${i}: issued ${credType} -> credId=${credResult.credentialId?.substring(0, 20)}...`);
        }
      }
      
      // === Apply status changes ===
      if (m.status === 'QUARANTINED') {
        try {
          await client.setStatus(materialId, 'QUARANTINED', metadataHash);
        } catch { /* Status change may fail */ }
      } else if (m.status === 'REVOKED') {
        try {
          await client.setStatus(materialId, 'REVOKED', metadataHash);
        } catch { /* Status change may fail */ }
      }
      
      // === Create transfers ===
      for (const transfer of m.transfers || []) {
        try {
          // initiateTransfer returns transferId
          const transferResult = await client.initiateTransfer(
            materialId,
            transfer.toOrg || 'TransferOrg',
            transfer.shipmentHash || metadataHash
          );
          
          // Accept transfer only if dataset says it's accepted
          // CRITICAL: Use transferId, not materialId
          if (transfer.accepted && transferResult.transferId) {
            await client.acceptTransfer(transferResult.transferId);
          }
          // If not accepted, leave as pending (TRANSFER_PENDING anomaly)
        } catch { /* Transfer may fail if already transferred */ }
      }
      
      // Query actual contract verification
      const verifyResult = await client.verifyMaterial(materialId);
      
      // === Full verification: on-chain + ACTUAL artifact integrity check ===
      let fullPass = verifyResult.pass;
      const fullReasons = [...verifyResult.reasons];
      
      // Perform REAL artifact integrity verification
      for (const cred of creds(m)) {
        if (cred.artifactRef?.hash) {
          const artifactIntegrityResult = await verifyArtifactIntegrity(
            dataDir, 
            m.materialId || `dataset-${i}`,
            cred.artifactRef
          );
          
          if (!artifactIntegrityResult.valid) {
            fullPass = false;
            fullReasons.push(
              artifactIntegrityResult.reason === 'ARTIFACT_UNAVAILABLE'
                ? 'ARTIFACT_UNAVAILABLE'
                : 'ARTIFACT_TAMPERED'
            );
          }
        }
      }
      
      predictions.push({
        material: m,
        materialId,
        onChainPass: verifyResult.pass,
        onChainReasons: verifyResult.reasons,
        fullPass,
        fullReasons,
      });
      
    } catch (error) {
      console.log(`  ‚ö†Ô∏è  Failed to materialize material ${i}: ${(error as Error).message}`);
    }
    
    if ((i + 1) % 20 === 0) {
      console.log(`  ${i + 1}/${materials.length} materialized`);
    }
  }
  
  console.log(`  Materialized ${predictions.length}/${materials.length} materials`);
  
  // Build confusion matrices from actual predictions
  const onChainResults: AnomalyDetectionResult[] = [];
  const fullResults: AnomalyDetectionResult[] = [];
  
  for (const anomalyType of anomalyTypes) {
    // On-chain confusion matrix
    // CRITICAL: Detection is based on REASON CODES ONLY, not pass/fail status
    // This prevents counting all anomalies as detected when contract fails for one reason
    let tp = 0, tn = 0, fp = 0, fn = 0;
    
    for (const p of predictions) {
      const groundTruthPositive = detectGroundTruthAnomaly(p.material, anomalyType);
      // Prediction: did the contract return THIS SPECIFIC anomaly reason?
      const onChainDetected = p.onChainReasons.some(r => reasonMatchesAnomaly(r, anomalyType));
      
      if (groundTruthPositive && onChainDetected) tp++;
      else if (!groundTruthPositive && !onChainDetected) tn++;
      else if (!groundTruthPositive && onChainDetected) fp++;
      else if (groundTruthPositive && !onChainDetected) fn++;
    }
    
    onChainResults.push({
      anomalyType,
      confusionMatrix: buildConfusionMatrix(tp, tn, fp, fn),
    });
    
    // Full verification confusion matrix
    tp = 0; tn = 0; fp = 0; fn = 0;
    
    for (const p of predictions) {
      const groundTruthPositive = detectGroundTruthAnomaly(p.material, anomalyType);
      // Prediction: did full verifier return THIS SPECIFIC anomaly reason?
      const fullDetected = p.fullReasons.some(r => reasonMatchesAnomaly(r, anomalyType));
      
      if (groundTruthPositive && fullDetected) tp++;
      else if (!groundTruthPositive && !fullDetected) tn++;
      else if (!groundTruthPositive && fullDetected) fp++;
      else if (groundTruthPositive && !fullDetected) fn++;
    }
    
    fullResults.push({
      anomalyType,
      confusionMatrix: buildConfusionMatrix(tp, tn, fp, fn),
    });
  }
  
  // Print results
  console.log('\n  === LIVE Confusion Matrices (Actual Contract Outputs) ===');
  console.log('  On-chain Detection:');
  for (const r of onChainResults) {
    const m = r.confusionMatrix;
    console.log(`    ${r.anomalyType.padEnd(20)}: TPR=${(m.tpr * 100).toFixed(1).padStart(5)}%, FPR=${(m.fpr * 100).toFixed(1).padStart(5)}%, F1=${(m.f1Score * 100).toFixed(1).padStart(5)}%`);
  }
  
  console.log('  Full Verification:');
  for (const r of fullResults) {
    const m = r.confusionMatrix;
    console.log(`    ${r.anomalyType.padEnd(20)}: TPR=${(m.tpr * 100).toFixed(1).padStart(5)}%, FPR=${(m.fpr * 100).toFixed(1).padStart(5)}%, F1=${(m.f1Score * 100).toFixed(1).padStart(5)}%`);
  }
  
  return { onChain: onChainResults, full: fullResults };
}

/**
 * Verify artifact integrity by hashing actual artifact bytes and comparing to stored hash.
 * This is REAL verification, not dataset-driven labeling.
 */
async function verifyArtifactIntegrity(
  dataDir: string,
  datasetMaterialId: string,
  artifactRef: { cid?: string; hash?: string; tampered?: boolean }
): Promise<{ valid: boolean; reason?: string }> {
  if (!artifactRef.hash) {
    return { valid: true }; // No hash to verify
  }
  
  // Try to find artifact file in dataset
  // Naming convention: data/artifacts/{materialId}_{credentialIndex}.bin or data/artifacts/{cid}.bin
  const sanitizedId = datasetMaterialId.replace(/[:/]/g, '_');
  const possiblePaths = [
    path.join(dataDir, 'artifacts', `${sanitizedId}.bin`),
    path.join(dataDir, 'artifacts', `${artifactRef.cid?.replace(/[:/]/g, '_')}.bin`),
    path.join(dataDir, 'artifacts', sanitizedId),
  ];
  
  let artifactBytes: Buffer | null = null;
  for (const p of possiblePaths) {
    if (fs.existsSync(p)) {
      artifactBytes = fs.readFileSync(p);
      break;
    }
  }
  
  if (!artifactBytes) {
    // Fail closed: if we cannot retrieve the artifact, we cannot verify integrity.
    // This avoids optimistic bias in paper results.
    return { valid: false, reason: 'ARTIFACT_UNAVAILABLE' };
  }
  
  // Compute actual hash
  const computedHash = crypto.createHash('sha256').update(artifactBytes).digest('hex');
  
  // Compare to stored hash
  const storedHash = artifactRef.hash.toLowerCase();
  const computedHashLower = computedHash.toLowerCase();
  
  if (storedHash !== computedHashLower) {
    return { valid: false, reason: 'HASH_MISMATCH' };
  }
  
  return { valid: true };
}

/**
 * Check if a verification reason code matches an anomaly type.
 * Uses EXACT code matching to prevent false positives.
 * 
 * Maps contract reason codes to anomaly types. If your contract uses
 * different codes, update the aliases here.
 */
function reasonMatchesAnomaly(reason: string, anomalyType: string): boolean {
  const code = reason.toUpperCase().trim();
  
  // Exact match first
  if (code === anomalyType) return true;
  
  // Map contract-specific aliases to anomaly types
  switch (anomalyType) {
    case 'QC_EXPIRED':
      return code === 'QC_EXPIRED' || code === 'CREDENTIAL_EXPIRED' || code === 'QC_INVALID_EXPIRED';
    case 'QC_MISSING':
      return code === 'QC_MISSING' || code === 'NO_QC' || code === 'MISSING_QC' || code === 'QC_REQUIRED';
    case 'MATERIAL_REVOKED':
      return code === 'MATERIAL_REVOKED' || code === 'REVOKED' || code === 'STATUS_REVOKED';
    case 'MATERIAL_QUARANTINED':
      return code === 'MATERIAL_QUARANTINED' || code === 'QUARANTINED' || code === 'STATUS_QUARANTINED';
    case 'TRANSFER_PENDING':
      return code === 'TRANSFER_PENDING' || code === 'PENDING_TRANSFER' || code === 'UNACCEPTED_TRANSFER';
    case 'ARTIFACT_TAMPERED':
      return code === 'ARTIFACT_TAMPERED' || code === 'HASH_MISMATCH' || code === 'INTEGRITY_FAILED';
    default:
      return false;
  }
}

/**
 * Ground truth: does the material actually have this anomaly?
 * 
 * IMPORTANT: Ground truth must align with the verifier policy being evaluated.
 * For QC_EXPIRED, we use "latest QC expired" (not "any QC expired") because
 * the contract's policy is "latest QC controls validity."
 */
function detectGroundTruthAnomaly(material: any, anomalyType: string): boolean {
  switch (anomalyType) {
    case 'QC_EXPIRED':
      // Align to verifier policy: latest QC controls validity
      return groundTruthLatestQCExpired(material);
    case 'QC_MISSING':
      return !creds(material).some((c: any) => c.credentialType === 'QC_MYCO');
    case 'MATERIAL_REVOKED':
      return material.status === 'REVOKED';
    case 'MATERIAL_QUARANTINED':
      return material.status === 'QUARANTINED';
    case 'TRANSFER_PENDING':
      return transfers(material).some((t: any) => !t.accepted);
    case 'ARTIFACT_TAMPERED':
      return creds(material).some((c: any) => c.artifactRef?.tampered);
    default:
      return false;
  }
}

/**
 * Ground truth for QC_EXPIRED aligned to "latest-QC-only" policy.
 * Returns true only if the LATEST QC credential is expired.
 */
function groundTruthLatestQCExpired(material: any): boolean {
  const qcCreds = creds(material).filter((c: any) => c.credentialType === 'QC_MYCO');
  if (qcCreds.length === 0) return false;
  
  const latestQC = qcCreds.reduce((latest: any, c: any) =>
    !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);
  
  return latestQC?.expired === true;
}

/**
 * Simulate on-chain contract detection for a specific anomaly type.
 * The contract CAN detect: QC_EXPIRED, QC_MISSING, MATERIAL_REVOKED, MATERIAL_QUARANTINED, TRANSFER_PENDING
 * The contract CANNOT detect: ARTIFACT_TAMPERED (off-chain only)
 */
function simulateOnChainDetection(material: any, anomalyType: string): boolean {
  switch (anomalyType) {
    case 'QC_EXPIRED': {
      const qcCreds = creds(material).filter((c: any) => c.credentialType === 'QC_MYCO');
      if (qcCreds.length === 0) return false;

      const latestQC = qcCreds.reduce((latest: any, c: any) =>
        !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);

      return latestQC?.expired === true;
    }
    case 'QC_MISSING':
      return !creds(material).some((c: any) => c.credentialType === 'QC_MYCO');
    case 'MATERIAL_REVOKED':
      return material.status === 'REVOKED';
    case 'MATERIAL_QUARANTINED':
      return material.status === 'QUARANTINED';
    case 'TRANSFER_PENDING':
      return transfers(material).some((t: any) => !t.accepted);
    case 'ARTIFACT_TAMPERED':
      return false; // on-chain cannot detect artifact tampering
    default:
      return false;
  }
}

/**
 * Simulate full verification detection (on-chain + artifact integrity).
 * Full verifier CAN detect all anomaly types including ARTIFACT_TAMPERED.
 */
function simulateFullDetection(material: any, anomalyType: string): boolean {
  if (anomalyType === 'ARTIFACT_TAMPERED') {
    // Full verifier checks artifact hashes against stored commitments
    return creds(material).some((c: any) => c.artifactRef?.tampered);
  }
  // For all other anomalies, full verifier has same detection as on-chain
  return simulateOnChainDetection(material, anomalyType);
}

/**
 * Compute binary PASS/FAIL classification confusion matrix.
 * Positive class = FAIL (invalid material)
 */
function computeBinaryClassification(materials: any[], mode: 'onchain' | 'full'): ConfusionMatrix {
  let tp = 0, tn = 0, fp = 0, fn = 0;
  
  for (const m of materials) {
    // Ground truth: is this material actually invalid?
    const groundTruthInvalid = isGroundTruthInvalid(m, mode);
    
    // Prediction: would the verifier say FAIL?
    const predictedInvalid = isPredictedInvalid(m, mode);
    
    if (groundTruthInvalid && predictedInvalid) tp++;
    else if (!groundTruthInvalid && !predictedInvalid) tn++;
    else if (!groundTruthInvalid && predictedInvalid) fp++;
    else if (groundTruthInvalid && !predictedInvalid) fn++;
  }
  
  return buildConfusionMatrix(tp, tn, fp, fn);
}

function isGroundTruthInvalid(m: any, mode: 'onchain' | 'full'): boolean {
  // Material is invalid if ANY anomaly is present
  // IMPORTANT: Use latest-QC-only policy for expiry (aligned with verifier behavior)
  const hasStatusIssue = m.status !== 'ACTIVE';
  const missingIdentity = !creds(m).some((c: any) => c.credentialType === 'IDENTITY' && !c.revoked);
  const missingQC = !creds(m).some((c: any) => c.credentialType === 'QC_MYCO');
  
  // Use latest-QC-only policy (not "any QC expired")
  const expiredLatestQC = groundTruthLatestQCExpired(m);
  
  const pendingTransfer = transfers(m).some((t: any) => !t.accepted);
  const tamperedArtifact = creds(m).some((c: any) => c.artifactRef?.tampered);
  
  if (mode === 'onchain') {
    return hasStatusIssue || missingIdentity || missingQC || expiredLatestQC || pendingTransfer;
  } else {
    return hasStatusIssue || missingIdentity || missingQC || expiredLatestQC || pendingTransfer || tamperedArtifact;
  }
}

function isPredictedInvalid(m: any, mode: 'onchain' | 'full'): boolean {
  // Simulate what the verifier would actually return
  // This should match the contract/CLI logic
  const hasStatusIssue = m.status !== 'ACTIVE';
  const missingIdentity = !creds(m).some((c: any) => c.credentialType === 'IDENTITY' && !c.revoked);
  const missingQC = !creds(m).some((c: any) => c.credentialType === 'QC_MYCO');
  
  // Check latest QC expiry (contract uses latest-QC-only policy)
  const qcCreds = creds(m).filter((c: any) => c.credentialType === 'QC_MYCO');
  let expiredLatestQC = false;
  if (qcCreds.length > 0) {
    const latestQC = qcCreds.reduce((latest: any, c: any) => 
      !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);
    expiredLatestQC = latestQC?.expired === true;
  }
  
  const pendingTransfer = transfers(m).some((t: any) => !t.accepted);
  
  if (mode === 'onchain') {
    return hasStatusIssue || missingIdentity || missingQC || expiredLatestQC || pendingTransfer;
  } else {
    const tamperedArtifact = creds(m).some((c: any) => c.artifactRef?.tampered);
    return hasStatusIssue || missingIdentity || missingQC || expiredLatestQC || pendingTransfer || tamperedArtifact;
  }
}

function detectAnomaly(material: any, anomalyType: string): boolean {
  switch (anomalyType) {
    case 'QC_EXPIRED':
      return creds(material).some((c: any) => c.credentialType === 'QC_MYCO' && c.expired);
    case 'QC_MISSING':
      return !creds(material).some((c: any) => c.credentialType === 'QC_MYCO');
    case 'MATERIAL_REVOKED':
      return material.status === 'REVOKED';
    case 'MATERIAL_QUARANTINED':
      return material.status === 'QUARANTINED';
    case 'TRANSFER_PENDING':
      return transfers(material).some((t: any) => !t.accepted);
    case 'ARTIFACT_TAMPERED':
      return creds(material).some((c: any) => c.artifactRef?.tampered);
    default:
      return false;
  }
}

function buildConfusionMatrix(tp: number, tn: number, fp: number, fn: number): ConfusionMatrix {
  const total = tp + tn + fp + fn;
  const tpr = tp + fn > 0 ? tp / (tp + fn) : 0;           // Sensitivity / Recall
  const tnr = tn + fp > 0 ? tn / (tn + fp) : 0;           // Specificity
  const fpr = fp + tn > 0 ? fp / (fp + tn) : 0;           // Fall-out
  const fnr = fn + tp > 0 ? fn / (fn + tp) : 0;           // Miss rate
  const precision = tp + fp > 0 ? tp / (tp + fp) : 0;     // Positive predictive value
  const accuracy = total > 0 ? (tp + tn) / total : 0;
  const recall = tpr;
  const f1Score = precision + recall > 0 ? 2 * (precision * recall) / (precision + recall) : 0;
  
  return {
    truePositive: tp,
    trueNegative: tn,
    falsePositive: fp,
    falseNegative: fn,
    tpr,
    tnr,
    fpr,
    fnr,
    precision,
    accuracy,
    f1Score,
  };
}

// ==================== Baseline Comparisons ====================

/**
 * Baseline comparisons using THEORETICAL/SIMULATED values.
 * 
 * NOTE: These baselines are NOT measured from actual systems.
 * They represent theoretical performance based on:
 * - Published benchmarks for PostgreSQL, Hyperledger, etc.
 * - Architectural analysis of each approach
 * 
 * For paper: clearly label these as "theoretical baselines" or
 * "estimated from published benchmarks" in the Results section.
 */
async function runBaselineComparisons(): Promise<BaselineResult[]> {
  console.log('\n‚öñÔ∏è Running Baseline Comparisons (THEORETICAL/SIMULATED)...');
  console.log('  NOTE: Baselines are theoretical estimates, not measured from actual systems');
  
  const baselines: BaselineResult[] = [];
  
  // Baseline 1: Centralized DB + Signed Audit Log (theoretical)
  console.log('  1. Centralized DB + Signed Audit Log [theoretical]');
  const centralizedLatencies: number[] = [];
  for (let i = 0; i < 500; i++) {
    centralizedLatencies.push(simulateLatency(5, 2)); // Much faster writes
  }
  
  baselines.push({
    name: 'Centralized DB + Audit Log [theoretical]',
    description: 'PostgreSQL with append-only signed audit log (estimated)',
    latencyMs: calculateStats(centralizedLatencies),
    throughputOps: 2500, // Higher throughput (estimated)
    securityScore: 45,   // Lower security (single point of trust)
    integrityGuarantee: 'Operator-dependent; audit log can be truncated by admin',
  });
  
  // Baseline 2: On-chain Only (no artifact integrity) [theoretical]
  console.log('  2. On-chain Only (no artifact checks) [theoretical]');
  const onChainOnlyLatencies: number[] = [];
  for (let i = 0; i < 500; i++) {
    onChainOnlyLatencies.push(simulateLatency(8, 3)); // Fast reads
  }
  
  baselines.push({
    name: 'On-chain Only [theoretical]',
    description: 'Smart contract verification without off-chain artifact integrity (estimated)',
    latencyMs: calculateStats(onChainOnlyLatencies),
    throughputOps: 850, // Estimated
    securityScore: 75,   // Good but misses tampering
    integrityGuarantee: 'Immutable on-chain state; artifacts can be tampered undetected',
  });
  
  // BioPassport Full (our approach) - note: in --live mode, this is measured
  console.log('  3. BioPassport Full Verification [simulated baseline]');
  const biopassportLatencies: number[] = [];
  for (let i = 0; i < 500; i++) {
    biopassportLatencies.push(simulateLatency(25, 10)); // Includes artifact check
  }
  
  baselines.push({
    name: 'BioPassport Full',
    description: 'On-chain policy + off-chain artifact integrity verification',
    latencyMs: calculateStats(biopassportLatencies),
    throughputOps: 420,
    securityScore: 95,   // High security
    integrityGuarantee: 'Immutable on-chain state + tamper-evident artifact hashing',
  });
  
  // Print comparison table
  console.log('\n  Baseline Comparison:');
  console.log('  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê');
  console.log('  ‚îÇ Approach                    ‚îÇ p50 (ms) ‚îÇ Ops/sec  ‚îÇ Security ‚îÇ');
  console.log('  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§');
  for (const b of baselines) {
    const name = b.name.padEnd(27);
    const p50 = b.latencyMs.p50.toFixed(1).padStart(8);
    const ops = b.throughputOps.toString().padStart(8);
    const sec = (b.securityScore + '%').padStart(8);
    console.log(`  ‚îÇ ${name} ‚îÇ ${p50} ‚îÇ ${ops} ‚îÇ ${sec} ‚îÇ`);
  }
  console.log('  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò');
  
  return baselines;
}

// ==================== Ablation Studies ====================

/**
 * Ablation studies using MEASURED predictions from dataset.
 * 
 * Each ablation disables a specific security feature and measures
 * the increase in false accepts (materials that SHOULD fail but now PASS).
 * 
 * NO FUDGE FACTORS - all results are computed from actual dataset state.
 * 
 * For meaningful ablations, the dataset generator should include:
 * - QC replay scenarios (multiple QC credentials, latest expired)
 * - Issuer revocation scenarios (credentials issued after revocation)
 * - Status abuse scenarios (quarantined/revoked materials)
 */
function runAblationStudies(dataDir: string): AblationResult[] {
  console.log('\nüî¨ Running Ablation Studies (Measured, No Fudge Factors)...');
  
  const ablations: AblationResult[] = [];
  
  // Load adversarial dataset for ablation analysis
  const materialsPath = path.join(dataDir, 'adversarial', 'materials.json');
  const materials = JSON.parse(fs.readFileSync(materialsPath, 'utf8'));
  
  // === BASELINE: Full BioPassport verification (latest-QC-only policy) ===
  const baselineFails = materials.filter((m: any) => {
    return !verifyWithFullPolicy(m);
  }).length;
  const baselineFailRate = baselineFails / materials.length;
  const baselinePassRate = 1 - baselineFailRate;
  
  console.log(`  Baseline: ${baselineFails}/${materials.length} FAIL (${(baselineFailRate * 100).toFixed(1)}%)`);
  
  // === ABLATION 1: Latest-QC-only policy OFF ===
  // Accept ANY valid QC credential, not just the latest one
  console.log('  1. Ablation: Latest-QC-only policy OFF');
  const ablation1Passes = materials.filter((m: any) => {
    return verifyWithAnyQCPolicy(m);
  }).length;
  const ablation1PassRate = ablation1Passes / materials.length;
  
  // Count materials that SHOULD fail (have expired latest QC) but now PASS (have older valid QC)
  const qcReplayVulnerable = materials.filter((m: any) => {
    const qcCreds = creds(m).filter((c: any) => c.credentialType === 'QC_MYCO');
    if (qcCreds.length < 2) return false;
    // Has multiple QCs, latest is expired, but older one is valid
    const latestQC = qcCreds.reduce((latest: any, c: any) => 
      !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);
    const hasOlderValidQC = qcCreds.some((c: any) => c !== latestQC && !c.expired);
    return latestQC?.expired && hasOlderValidQC;
  }).length;
  
  ablations.push({
    name: 'Latest-QC-only OFF',
    description: 'Accept any QC credential, not just the latest valid one',
    featureDisabled: 'Latest QC credential enforcement',
    baselinePassRate,
    ablatedPassRate: ablation1PassRate,
    falseAcceptIncrease: ablation1PassRate - baselinePassRate,
    securityImpact: `${qcReplayVulnerable} materials vulnerable to QC replay attack`,
  });
  
  // === ABLATION 2: Artifact integrity check OFF ===
  // No off-chain artifact hash verification
  // NOTE: Ablations for "Issuer revocation timestamp" and "Authority-only status control"
  // were removed because those features are NOT implemented in the current contract.
  // Only ablate features that actually exist in the system.
  console.log('  2. Ablation: Artifact integrity check OFF');
  
  const tamperedCount = materials.filter((m: any) => 
    creds(m).some((c: any) => c.artifactRef?.tampered)
  ).length;
  
  const ablation4Passes = materials.filter((m: any) => {
    return verifyIgnoringArtifacts(m);
  }).length;
  const ablation4PassRate = ablation4Passes / materials.length;
  
  ablations.push({
    name: 'Artifact Integrity Check OFF',
    description: 'Off-chain artifact hash verification disabled',
    featureDisabled: 'Artifact integrity verification',
    baselinePassRate,
    ablatedPassRate: ablation4PassRate,
    falseAcceptIncrease: ablation4PassRate - baselinePassRate,
    securityImpact: `${tamperedCount} tampered artifacts would go undetected`,
  });
  
  // Print ablation table
  console.log('\n  Ablation Study Results (Measured):');
  console.log('  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê');
  console.log('  ‚îÇ Feature Disabled               ‚îÇ Baseline ‚îÇ Ablated  ‚îÇ Œî False+  ‚îÇ');
  console.log('  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§');
  for (const a of ablations) {
    const name = a.name.padEnd(30);
    const base = (a.baselinePassRate * 100).toFixed(1).padStart(7) + '%';
    const ablated = (a.ablatedPassRate * 100).toFixed(1).padStart(7) + '%';
    const delta = ('+' + (a.falseAcceptIncrease * 100).toFixed(1) + '%').padStart(9);
    console.log(`  ‚îÇ ${name} ‚îÇ ${base} ‚îÇ ${ablated} ‚îÇ ${delta} ‚îÇ`);
  }
  console.log('  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò');
  
  return ablations;
}

// Verification helper functions for ablations

function verifyWithFullPolicy(m: any): boolean {
  const hasIdentity = creds(m).some((c: any) => c.credentialType === 'IDENTITY' && !c.revoked);
  const isActive = m.status === 'ACTIVE';
  const noPending = transfers(m).every((t: any) => t.accepted);
  const noTamper = !creds(m).some((c: any) => c.artifactRef?.tampered);
  
  // Latest-QC-only policy (using normalized date comparison)
  const qcCreds = creds(m).filter((c: any) => c.credentialType === 'QC_MYCO');
  if (qcCreds.length === 0) return false;
  const latestQC = qcCreds.reduce((latest: any, c: any) => 
    !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);
  const hasValidLatestQC = latestQC && !latestQC.expired && !latestQC.revoked;
  
  return hasIdentity && hasValidLatestQC && isActive && noPending && noTamper;
}

function verifyWithAnyQCPolicy(m: any): boolean {
  const hasIdentity = creds(m).some((c: any) => c.credentialType === 'IDENTITY' && !c.revoked);
  // Accept ANY valid QC, not just latest
  const hasAnyValidQC = creds(m).some((c: any) => 
    c.credentialType === 'QC_MYCO' && !c.expired && !c.revoked
  );
  const isActive = m.status === 'ACTIVE';
  const noPending = transfers(m).every((t: any) => t.accepted);
  const noTamper = !creds(m).some((c: any) => c.artifactRef?.tampered);
  
  return hasIdentity && hasAnyValidQC && isActive && noPending && noTamper;
}

function verifyIgnoringIssuerRevocation(m: any): boolean {
  const hasIdentity = creds(m).some((c: any) => c.credentialType === 'IDENTITY' && !c.revoked);
  const isActive = m.status === 'ACTIVE';
  const noPending = transfers(m).every((t: any) => t.accepted);
  const noTamper = !creds(m).some((c: any) => c.artifactRef?.tampered);
  
  // Accept QC even if issuer was revoked (ignore issuerRevokedAt)
  const qcCreds = creds(m).filter((c: any) => c.credentialType === 'QC_MYCO');
  if (qcCreds.length === 0) return false;
  const latestQC = qcCreds.reduce((latest: any, c: any) => 
    !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);
  const hasValidQC = latestQC && !latestQC.expired; // Ignore revoked status
  
  return hasIdentity && hasValidQC && isActive && noPending && noTamper;
}

function verifyIgnoringStatus(m: any): boolean {
  const hasIdentity = creds(m).some((c: any) => c.credentialType === 'IDENTITY' && !c.revoked);
  const noPending = transfers(m).every((t: any) => t.accepted);
  const noTamper = !creds(m).some((c: any) => c.artifactRef?.tampered);
  
  // Ignore status check entirely
  const qcCreds = creds(m).filter((c: any) => c.credentialType === 'QC_MYCO');
  if (qcCreds.length === 0) return false;
  const latestQC = qcCreds.reduce((latest: any, c: any) => 
    !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);
  const hasValidLatestQC = latestQC && !latestQC.expired && !latestQC.revoked;
  
  return hasIdentity && hasValidLatestQC && noPending && noTamper;
}

function verifyIgnoringArtifacts(m: any): boolean {
  const hasIdentity = creds(m).some((c: any) => c.credentialType === 'IDENTITY' && !c.revoked);
  const isActive = m.status === 'ACTIVE';
  const noPending = transfers(m).every((t: any) => t.accepted);
  // No artifact tamper check
  
  const qcCreds = creds(m).filter((c: any) => c.credentialType === 'QC_MYCO');
  if (qcCreds.length === 0) return false;
  const latestQC = qcCreds.reduce((latest: any, c: any) => 
    !latest || isDateAfter(c.issuedAt, latest.issuedAt) ? c : latest, null);
  const hasValidLatestQC = latestQC && !latestQC.expired && !latestQC.revoked;
  
  return hasIdentity && hasValidLatestQC && isActive && noPending;
}

// ==================== Scaling Test ====================

/**
 * Scaling test measures verify and query latency as material count grows.
 * 
 * In --live mode: Actually deploys contract and measures real latencies
 * In --simulate mode: Uses theoretical O(1) model with measured baseline
 * 
 * FIXED: Now registers incrementally (count - previousCount) to avoid
 * cumulative over-registration bug.
 */
async function runScalingTest(client?: BlockchainClient): Promise<BenchmarkReport['scalingTest']> {
  const isLive = BENCHMARK_MODE === 'live' && client;
  
  console.log(`\nüìê Running Scaling Test (${isLive ? 'LIVE' : 'SIMULATED'})...`);
  
  // For live mode, use smaller counts; for simulate, go to 10k
  const materialCounts = isLive 
    ? [100, 250, 500, 1000, 2000]  // Practical live limits
    : [100, 500, 1000, 2500, 5000, 10000];
  
  const verifyLatencyMs: number[] = [];
  const queryLatencyMs: number[] = [];
  
  if (isLive && client) {
    console.log('  [LIVE] Measuring actual PureChain latencies...');
    
    // Track all registered material IDs across scaling levels
    const allMaterialIds: string[] = [];
    let previousCount = 0;
    
    for (const targetCount of materialCounts) {
      // Register only the INCREMENTAL materials needed to reach targetCount
      const toRegister = targetCount - previousCount;
      console.log(`  Registering ${toRegister} materials with IDENTITY + QC (total: ${targetCount})...`);
      
      for (let i = 0; i < toRegister; i++) {
        const hash = crypto.createHash('sha256').update(`scaling-${targetCount}-${i}-${Date.now()}`).digest('hex');
        const result = await client.registerMaterial('CELL_LINE', hash);
        
        // CRITICAL: Issue IDENTITY + QC credentials for realistic verify path
        // Without these, verify returns MISSING_IDENTITY/QC_MISSING (fast fail, not representative)
        await client.issueCredential(
          result.materialId, 'IDENTITY', hash,
          Math.floor(Date.now() / 1000) + 86400 * 90,
          `s3://scaling/${i}`, hash, ''
        );
        await client.issueCredential(
          result.materialId, 'QC_MYCO', hash,
          Math.floor(Date.now() / 1000) + 86400 * 90,
          `s3://scaling/${i}-qc`, hash, ''
        );
        
        allMaterialIds.push(result.materialId);
        
        if ((i + 1) % 100 === 0) {
          process.stdout.write(`    ${i + 1}/${toRegister}\r`);
        }
      }
      
      previousCount = targetCount;
      
      // Measure verify latency (sample 50 random materials from ALL registered)
      const verifyLatencies: number[] = [];
      for (let i = 0; i < Math.min(50, allMaterialIds.length); i++) {
        const idx = Math.floor(Math.random() * allMaterialIds.length);
        const result = await client.verifyMaterial(allMaterialIds[idx]);
        verifyLatencies.push(result.latencyMs);
      }
      
      // Measure query latency (history query)
      const queryLatencies: number[] = [];
      for (let i = 0; i < Math.min(50, allMaterialIds.length); i++) {
        const idx = Math.floor(Math.random() * allMaterialIds.length);
        const result = await client.getHistory(allMaterialIds[idx]);
        queryLatencies.push(result.latencyMs);
      }
      
      const avgVerify = verifyLatencies.reduce((a, b) => a + b, 0) / verifyLatencies.length;
      const avgQuery = queryLatencies.reduce((a, b) => a + b, 0) / queryLatencies.length;
      
      verifyLatencyMs.push(avgVerify);
      queryLatencyMs.push(avgQuery);
      
      console.log(`  ${targetCount.toString().padStart(5)} materials: verify=${avgVerify.toFixed(1)}ms, query=${avgQuery.toFixed(1)}ms`);
    }
  } else {
    console.log('  [SIMULATED] Using theoretical model...');
    console.log('  NOTE: For paper results, run with --live flag');
    console.log('  NOTE: Verify is O(1) mapping lookup; history query returns full list (not paginated)');
    
    for (const count of materialCounts) {
      // Theoretical model for verify: O(1) mapping lookup with minimal state trie overhead
      const baseVerify = 8; // Baseline from single-material measurement
      const overhead = Math.log10(count) * 0.3; // Minimal state trie depth increase
      verifyLatencyMs.push(baseVerify + overhead);
      
      // History query: NOT O(1) - returns full history, grows with history length
      // This is a limitation; for true O(1), implement getHistorySlice(offset, limit)
      const baseQuery = 5;
      const historyGrowth = Math.log10(count) * 0.5; // Grows with history size
      queryLatencyMs.push(baseQuery + historyGrowth);
      
      console.log(`  ${count.toString().padStart(5)} materials: verify=${(baseVerify + overhead).toFixed(1)}ms, query=${(baseQuery + historyGrowth).toFixed(1)}ms [simulated]`);
    }
  }
  
  return { materialCounts, verifyLatencyMs, queryLatencyMs };
}

// ==================== Reproducibility ====================

/**
 * Collect reproducibility metadata for paper-grade reporting.
 * Includes git hash, node version, platform, and dataset checksum.
 */
function collectReproducibilityMetadata(dataDir: string): ReproducibilityMetadata {
  let gitCommitHash = 'unknown';
  try {
    const { execSync } = require('child_process');
    gitCommitHash = execSync('git rev-parse HEAD', { encoding: 'utf8' }).trim().substring(0, 12);
  } catch { /* git not available */ }
  
  // Compute dataset checksum
  let datasetChecksum = 'unknown';
  try {
    const materialsPath = path.join(dataDir, 'normal', 'materials.json');
    if (fs.existsSync(materialsPath)) {
      const content = fs.readFileSync(materialsPath, 'utf8');
      datasetChecksum = crypto.createHash('sha256').update(content).digest('hex').substring(0, 16);
    }
  } catch { /* file not found */ }
  
  return {
    gitCommitHash,
    nodeVersion: process.version,
    platform: `${process.platform}-${process.arch}`,
    benchmarkMode: BENCHMARK_MODE,
    chainId: BENCHMARK_MODE === 'live' ? 'purechain-testnet' : undefined,
    networkName: BENCHMARK_MODE === 'live' ? 'PureChain Testnet' : undefined,
    rpcUrl: BENCHMARK_MODE === 'live' ? '[redacted]' : undefined,
    datasetChecksum,
  };
}

// ==================== Report Generation ====================

/**
 * Escape special LaTeX characters to prevent compilation errors.
 */
function latexEscape(s: string): string {
  return (s || '')
    .replace(/\\/g, '\\textbackslash{}')
    .replace(/&/g, '\\&')
    .replace(/%/g, '\\%')
    .replace(/\$/g, '\\$')
    .replace(/#/g, '\\#')
    .replace(/_/g, '\\_')
    .replace(/{/g, '\\{')
    .replace(/}/g, '\\}')
    .replace(/\^/g, '\\textasciicircum{}')
    .replace(/~/g, '\\textasciitilde{}');
}

function generateReport(report: BenchmarkReport, outputDir: string): void {
  console.log('\nüìù Generating Reports...');
  
  // Save JSON report
  const jsonPath = path.join(outputDir, 'benchmark-report.json');
  fs.writeFileSync(jsonPath, JSON.stringify(report, null, 2));
  console.log(`  Saved: ${jsonPath}`);
  
  // Generate LaTeX tables for paper
  const latexPath = path.join(outputDir, 'tables.tex');
  let latex = `% BioPassport Benchmark Results - Auto-generated
% Timestamp: ${report.timestamp}

% Table 1: Latency Distribution
\\begin{table}[h]
\\centering
\\caption{Operation Latency Distribution (ms)}
\\label{tab:latency}
\\begin{tabular}{lrrrrr}
\\toprule
Operation & p50 & p95 & p99 & Mean & Std \\\\
\\midrule
`;
  
  for (const [op, stats] of Object.entries(report.latency)) {
    const opName = op.replace(/([A-Z])/g, ' $1').trim();
    latex += `${opName} & ${stats.p50.toFixed(1)} & ${stats.p95.toFixed(1)} & ${stats.p99.toFixed(1)} & ${stats.mean.toFixed(1)} & ${stats.stdDev.toFixed(1)} \\\\\n`;
  }
  
  latex += `\\bottomrule
\\end{tabular}
\\end{table}

% Table 2: Throughput vs Concurrency
\\begin{table}[h]
\\centering
\\caption{Throughput Scaling}
\\label{tab:throughput}
\\begin{tabular}{rrrr}
\\toprule
Clients & Ops/sec & Avg Latency (ms) & p99 Latency (ms) \\\\
\\midrule
`;
  
  for (const t of report.throughput) {
    latex += `${t.concurrency} & ${t.opsPerSecond} & ${t.avgLatencyMs.toFixed(1)} & ${t.p99LatencyMs.toFixed(1)} \\\\\n`;
  }
  
  latex += `\\bottomrule
\\end{tabular}
\\end{table}

% Table 3: Baseline Comparison
\\begin{table}[h]
\\centering
\\caption{Baseline Comparison}
\\label{tab:baselines}
\\begin{tabular}{lrrrl}
\\toprule
Approach & p50 (ms) & Ops/sec & Security & Integrity Guarantee \\\\
\\midrule
`;
  
  for (const b of report.baselines) {
    latex += `${latexEscape(b.name)} & ${b.latencyMs.p50.toFixed(1)} & ${b.throughputOps} & ${b.securityScore}\\% & ${latexEscape(b.integrityGuarantee.substring(0, 30))}... \\\\\n`;
  }
  
  latex += `\\bottomrule
\\end{tabular}
\\end{table}

% Table 4: Ablation Study
\\begin{table}[h]
\\centering
\\caption{Ablation Study: Security Feature Impact}
\\label{tab:ablations}
\\begin{tabular}{lrrr}
\\toprule
Feature Disabled & Baseline & Ablated & False Accept Increase \\\\
\\midrule
`;
  
  for (const a of report.ablations) {
    latex += `${latexEscape(a.name)} & ${(a.baselinePassRate * 100).toFixed(1)}\\% & ${(a.ablatedPassRate * 100).toFixed(1)}\\% & +${(a.falseAcceptIncrease * 100).toFixed(1)}\\% \\\\\n`;
  }
  
  latex += `\\bottomrule
\\end{tabular}
\\end{table}
`;
  
  fs.writeFileSync(latexPath, latex);
  console.log(`  Saved: ${latexPath}`);
  
  // Generate CSV for plotting
  const csvPath = path.join(outputDir, 'scaling.csv');
  let csv = 'materials,verify_latency_ms,query_latency_ms\n';
  for (let i = 0; i < report.scalingTest.materialCounts.length; i++) {
    csv += `${report.scalingTest.materialCounts[i]},${report.scalingTest.verifyLatencyMs[i].toFixed(2)},${report.scalingTest.queryLatencyMs[i].toFixed(2)}\n`;
  }
  fs.writeFileSync(csvPath, csv);
  console.log(`  Saved: ${csvPath}`);
}

// ==================== Main ====================

async function main(): Promise<void> {
  console.log('‚ïê'.repeat(70));
  console.log('  BIOPASSPORT COMPREHENSIVE BENCHMARK SUITE');
  console.log('  IEEE ICBC Best Paper-level Evaluation');
  console.log('‚ïê'.repeat(70));
  
  const args = process.argv.slice(2);
  const fullMode = args.includes('--full');
  const liveMode = args.includes('--live');
  const simulateMode = args.includes('--simulate');
  
  // Set global benchmark mode
  BENCHMARK_MODE = liveMode ? 'live' : 'simulate';
  
  // Default to simulate if neither specified
  if (!liveMode && !simulateMode) {
    console.log('\n‚ö†Ô∏è  No mode specified. Use --live for paper results, --simulate for development.');
    console.log('    Defaulting to --simulate mode.\n');
  }
  
  const iterations = fullMode ? 1000 : 500;
  const concurrencyLevels = fullMode ? [1, 5, 10, 20, 50, 100] : [1, 5, 20, 50];
  
  console.log(`\nBenchmark Mode: ${BENCHMARK_MODE.toUpperCase()}`);
  console.log(`Iterations: ${iterations}`);
  console.log(`Concurrency levels: ${concurrencyLevels.join(', ')}`);
  
  if (BENCHMARK_MODE === 'simulate') {
    console.log('\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê');
    console.log('‚îÇ WARNING: Running in SIMULATE mode                              ‚îÇ');
    console.log('‚îÇ Results are EMULATED, not measured on actual blockchain.       ‚îÇ');
    console.log('‚îÇ For paper submission, run with: --live                         ‚îÇ');
    console.log('‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò');
  }
  
  const dataDir = path.join(__dirname, 'data');
  const outputDir = path.join(__dirname, 'results');
  fs.mkdirSync(outputDir, { recursive: true });
  
  // Initialize blockchain client for live mode
  let client: BlockchainClient | undefined;
  if (BENCHMARK_MODE === 'live') {
    console.log('\nüîó Connecting to PureChain...');
    try {
      // Dynamic import to avoid requiring purechainlib in simulate mode
      const { createPureChainClient } = await import('../issuer-service/src/purechain-client');
      const pureChainClient = createPureChainClient();
      await pureChainClient.connect();
      await pureChainClient.deployContract();
      
      // Wrap PureChainClient to match BlockchainClient interface
      // 
      // CRITICAL ASSUMPTION: Write latency = time-to-finality
      // This assumes pureChainClient methods internally await tx.wait() or equivalent
      // to ensure the transaction is mined/committed before returning.
      // 
      // If pureChainClient returns before finality, these measurements represent
      // "submit latency" not "finality latency". Verify pureChainClient implementation
      // awaits receipt confirmation for accurate finality measurements.
      //
      // Read ops measure RPC round-trip time.
      client = {
        connect: async () => { await pureChainClient.connect(); },
        disconnect: async () => { await pureChainClient.disconnect(); },
        deployContract: async () => { return await pureChainClient.deployContract(); },
        
        // WRITE OPS: Measure time-to-finality (submit tx ‚Üí receipt mined)
        registerMaterial: async (type, hash) => {
          const t0 = performance.now();
          const result = await pureChainClient.registerMaterial(type, hash);
          // registerMaterial internally awaits receipt, so this is finality time
          const latencyMs = performance.now() - t0;
          return { 
            latencyMs, 
            materialId: (result.result as any)?.materialId || '' 
          };
        },
        
        issueCredential: async (materialId: string, credType: string, commitmentHash: string, validUntil: number, artifactCid: string, artifactHash: string, signatureRef: string) => {
          const t0 = performance.now();
          // Solidity signature: (materialId, credentialType, commitmentHash, validUntil, artifactRefs, signatureRef)
          // credentialType is now a string directly
          const result = await pureChainClient.issueCredential(
            materialId, 
            credType as 'IDENTITY' | 'QC_MYCO' | 'USAGE_RIGHTS',
            commitmentHash,
            validUntil,              // unix seconds
            artifactCid,
            artifactHash,
            signatureRef || ''
          );
          const latencyMs = performance.now() - t0;
          return { 
            latencyMs, 
            credentialId: (result.result as any)?.credentialId || '' 
          };
        },
        
        initiateTransfer: async (materialId: string, toOrg: string, shipmentHash: string) => {
          const t0 = performance.now();
          const result = await pureChainClient.transferMaterial(materialId, toOrg, shipmentHash);
          const latencyMs = performance.now() - t0;
          const transferId = (result.result as any)?.transferId || '';
          return { latencyMs, transferId };
        },
        
        acceptTransfer: async (transferId: string) => {
          const t0 = performance.now();
          await pureChainClient.acceptTransfer(transferId);
          const latencyMs = performance.now() - t0;
          return { latencyMs };
        },
        
        // READ OPS: Measure RPC round-trip
        verifyMaterial: async (materialId) => {
          const t0 = performance.now();
          const result = await pureChainClient.verifyMaterial(materialId);
          const latencyMs = performance.now() - t0;
          // Handle both string[] and {code}[] formats for reasons
          const reasons = (result.reasons ?? []).map((r: any) =>
            typeof r === 'string' ? r : (r.code ?? r.reason ?? r.message ?? '')
          ).filter((x: string) => x.length > 0);
          return { 
            latencyMs, 
            pass: result.pass, 
            reasons 
          };
        },
        
        // Note: Returns full history, not paginated. For true pagination, implement contract-level slicing.
        getHistory: async (materialId: string) => {
          const t0 = performance.now();
          const result = await pureChainClient.getHistory(materialId);
          const latencyMs = performance.now() - t0;
          return { latencyMs, count: Array.isArray(result) ? result.length : 0 };
        },
        
        // Status change method - uses setStatus with reasonHash
        setStatus: async (materialId: string, status: 'ACTIVE' | 'QUARANTINED' | 'REVOKED', reasonHash: string) => {
          const t0 = performance.now();
          await pureChainClient.setStatus(materialId, status, reasonHash);
          const latencyMs = performance.now() - t0;
          return { latencyMs };
        },
      };
      
      console.log('  ‚úì Connected to PureChain');
    } catch (error) {
      console.error('  ‚úó Failed to connect to PureChain:', (error as Error).message);
      console.log('  Falling back to simulate mode...');
      BENCHMARK_MODE = 'simulate';
    }
  }
  
  // Check for --live-confusion flag
  const liveConfusion = args.includes('--live-confusion');
  if (liveConfusion) {
    console.log('\nüî¨ Live confusion matrices enabled - will materialize data onto chain');
  }
  
  // Run all benchmarks
  const latency = await benchmarkLatency(iterations, client, dataDir);
  const throughput = await benchmarkThroughput(concurrencyLevels, client);
  const storage = analyzeStorage(dataDir);
  const confusionMatrices = await calculateConfusionMatrices(dataDir, client, liveConfusion);
  const baselines = await runBaselineComparisons();
  const ablations = runAblationStudies(dataDir);
  const scalingTest = await runScalingTest(client);
  
  // Disconnect client
  if (client) {
    await client.disconnect();
  }
  
  // Collect reproducibility metadata
  const reproducibility = collectReproducibilityMetadata(dataDir);
  
  // Build report
  const report: BenchmarkReport = {
    timestamp: new Date().toISOString(),
    reproducibility,
    config: {
      materialsCount: storage.materialsCount,
      concurrencyLevels,
      iterations,
    },
    latency,
    throughput,
    storage,
    confusionMatrices,
    baselines,
    ablations,
    scalingTest,
  };
  
  // Generate outputs
  generateReport(report, outputDir);
  
  console.log('\n' + '‚ïê'.repeat(70));
  console.log(`  BENCHMARK COMPLETE (${BENCHMARK_MODE.toUpperCase()} MODE)`);
  console.log('‚ïê'.repeat(70));
  console.log('\nOutputs:');
  console.log('  results/benchmark-report.json  - Full JSON report');
  console.log('  results/tables.tex             - LaTeX tables for paper');
  console.log('  results/scaling.csv            - Scaling data for plots');
  if (BENCHMARK_MODE === 'simulate') {
    console.log('\n‚ö†Ô∏è  Results are SIMULATED. For paper submission, run with --live');
  }
  console.log('‚ïê'.repeat(70));
}

main().catch(console.error);
